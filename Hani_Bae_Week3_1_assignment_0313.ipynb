{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pitapatat/wanted_pre_onboarding/blob/main/Hani_Bae_Week3_1_assignment_0313.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ugGR7pnI4WSe"
      },
      "source": [
        "# Week3_1 Assignment\n",
        "\n",
        "## [BASIC](#Basic) \n",
        "- 토크나이징이 완료된 위키 백과 코퍼스를 다운받고 **단어 사전을 구축하는 함수를 구현**할 수 있다.\n",
        "- `Skip-Gram` 방식의 학습 데이터 셋을 생성하는 **Dataset과 Dataloader 클래스를 구현**할 수 있다.\n",
        "- **Negative Sampling** 함수를 구현할 수 있다. \n",
        "\n",
        "\n",
        "## [CHALLENGE](#Challenge)\n",
        "- Skip-Gram을 학습 과정 튜토리얼을 따라하며, **Skip-Gram을 학습하는 클래스를 구현**할 수 있다. \n",
        "\n",
        "\n",
        "## [ADVANCED](#Advanced)\n",
        "- Skip-Gram 방식으로 word embedding을 학습하는 **Word2Vec 클래스를 구현**하고 실제로 학습할 수 있다.\n",
        "- 학습이 완료된 word embedding을 불러와 **Gensim 패키지를 사용해 유사한 단어**를 뽑을 수 있다. \n",
        "\n",
        "### Reference\n",
        "- [Skip-Gram negative sampling 한국어 튜토리얼](https://wikidocs.net/69141)\n",
        "    - (참고) 위 튜토리얼에서는 target word와 context word 페어의 레이블은 1로, target word와 negative sample word 페어의 레이블은 0이 되도록 학습 데이터를 구현해 binary classification을 구현한다. 하지만 우리는 word2vec 논문 방식을 그대로 따르기 위해 label을 생성하지 않고 대신 loss 함수를 변행해서 binary classification을 학습할 것이다. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:29:36.641276Z",
          "start_time": "2022-02-19T14:29:36.638642Z"
        },
        "id": "HlEy3xfY4WSh"
      },
      "outputs": [],
      "source": [
        "import os \n",
        "import sys\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from typing import List, Dict\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T12:50:41.644583Z",
          "start_time": "2022-02-19T12:50:41.642937Z"
        },
        "id": "cBrr7-gt4jnf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a1faea1-6281-4317-b80e-98f56bc344c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.17.0-py3-none-any.whl (3.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8 MB 6.9 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.4.0-py3-none-any.whl (67 kB)\n",
            "\u001b[K     |████████████████████████████████| 67 kB 6.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 71.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.47-py2.py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 66.8 MB/s \n",
            "\u001b[?25hCollecting tokenizers!=0.11.3,>=0.11.1\n",
            "  Downloading tokenizers-0.11.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.5 MB 54.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.63.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.5)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.4.0 pyyaml-6.0 sacremoses-0.0.47 tokenizers-0.11.6 transformers-4.17.0\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:26:59.276355Z",
          "start_time": "2022-02-19T14:26:58.411434Z"
        },
        "id": "6mC9lhsJ4WSh"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import SGD\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:30:05.586472Z",
          "start_time": "2022-02-19T14:30:05.583611Z"
        },
        "id": "17g7UZ5g4WSi"
      },
      "outputs": [],
      "source": [
        "# seed\n",
        "seed = 7777\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:30:06.721039Z",
          "start_time": "2022-02-19T14:30:06.717559Z"
        },
        "id": "v3UlC7Jn4WSi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d0f06ee-85ab-476a-b129-0ab9754ac95a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# available GPUs : 1\n",
            "GPU name : Tesla P100-PCIE-16GB\n",
            "cuda\n"
          ]
        }
      ],
      "source": [
        "# device type\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(f\"# available GPUs : {torch.cuda.device_count()}\")\n",
        "    print(f\"GPU name : {torch.cuda.get_device_name()}\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8sfv5KY4WSk"
      },
      "source": [
        "## Basic"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sHs8_LU04WSj"
      },
      "source": [
        "### 토크나이징이 완료된 위키 백과 코퍼스 다운로드 및 불용어 사전 크롤링\n",
        "- 나의 구글 드라이브에 데이터를 다운받아 영구적으로 사용할 수 있도록 하자. \n",
        "    - [데이터 다운로드 출처](https://ratsgo.github.io/embedding/downloaddata.html)\n",
        "- 다운받은 데이터는 토크나이징이 완료된 상태이지만 불용어를 포함하고 있다. 따라서 향후 불용어를 제거하기 위해 불용어 사전을 크롤링하자. \n",
        "    - [불용어 사전 출처](https://www.ranks.nl/stopwords/korean)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KYiz1fdNsAqp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3d7d1ba-b0f5-4737-e841-cd6698b7febc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd \"/content/drive/MyDrive\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sQoSsWcEvH3x",
        "outputId": "8e199cbd-41d2-4dea-d73d-d9a36968ee10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gdown\n",
        "### gdown accesse denied >> error 해결!!\n",
        "#https://github.com/wkentaro/gdown/issues/43\n",
        "!pip install --upgrade --no-cache-dir gdown"
      ],
      "metadata": {
        "id": "W39cP82ay13p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82d7db53-9227-4218-bee4-3ac94203d14d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gdown in /usr/local/lib/python3.7/dist-packages (4.2.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from gdown) (1.15.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from gdown) (4.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from gdown) (4.63.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from gdown) (3.6.0)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.7/dist-packages (from gdown) (2.23.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (3.0.4)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.7/dist-packages (4.2.2)\n",
            "Collecting gdown\n",
            "  Downloading gdown-4.4.0.tar.gz (14 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from gdown) (3.6.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from gdown) (4.6.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from gdown) (1.15.0)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.7/dist-packages (from gdown) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from gdown) (4.63.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (2021.10.8)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "Building wheels for collected packages: gdown\n",
            "  Building wheel for gdown (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gdown: filename=gdown-4.4.0-py3-none-any.whl size=14774 sha256=45e0f4fe84602a78672af1e82afcd474210ba4672011e66c10b54fd4ef91e472\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-9bo1zeu3/wheels/fb/c3/0e/c4d8ff8bfcb0461afff199471449f642179b74968c15b7a69c\n",
            "Successfully built gdown\n",
            "Installing collected packages: gdown\n",
            "  Attempting uninstall: gdown\n",
            "    Found existing installation: gdown 4.2.2\n",
            "    Uninstalling gdown-4.2.2:\n",
            "      Successfully uninstalled gdown-4.2.2\n",
            "Successfully installed gdown-4.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:27:11.886643Z",
          "start_time": "2022-02-19T14:27:11.884858Z"
        },
        "id": "4QPBJ6UZ4WSj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f22f89d5-272c-434e-d2f9-c42938bd7285"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/u/0/uc?id=1Ybp_DmzNEpsBrUKZ1-NoPDzCMO39f-fx\n",
            "To: /content/drive/MyDrive/tokenized.zip\n",
            "100% 873M/873M [00:04<00:00, 196MB/s]\n",
            "Archive:  tokenized.zip\n",
            "replace tokenized/korquad_mecab.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n",
            "  inflating: tokenized/korquad_mecab.txt  \n",
            "  inflating: tokenized/wiki_ko_mecab.txt  \n",
            "  inflating: tokenized/corpus_mecab_jamo.txt  \n",
            "  inflating: tokenized/ratings_okt.txt  \n",
            "  inflating: tokenized/ratings_khaiii.txt  \n",
            "  inflating: tokenized/ratings_hannanum.txt  \n",
            "  inflating: tokenized/ratings_soynlp.txt  \n",
            "  inflating: tokenized/ratings_mecab.txt  \n",
            "  inflating: tokenized/ratings_komoran.txt  \n"
          ]
        }
      ],
      "source": [
        "# 데이터 다운로드\n",
        "!gdown https://drive.google.com/u/0/uc?id=1Ybp_DmzNEpsBrUKZ1-NoPDzCMO39f-fx\n",
        "!unzip tokenized.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:27:15.633947Z",
          "start_time": "2022-02-19T14:27:13.829982Z"
        },
        "id": "cTHCHmO24WSj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8894efb3-3085-43b0-e666-17d7de208689"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/urllib3/connectionpool.py:847: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
            "  InsecureRequestWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Korean stop words: 677\n"
          ]
        }
      ],
      "source": [
        "# 한국어 불용어 리스트 크롤링\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "url = \"https://www.ranks.nl/stopwords/korean\"\n",
        "response = requests.get(url, verify = False)\n",
        "\n",
        "if response.status_code == 200:\n",
        "    soup = BeautifulSoup(response.text,'html.parser')\n",
        "    content = soup.select_one('#article178ebefbfb1b165454ec9f168f545239 > div.panel-body > table > tbody > tr')\n",
        "    stop_words=[]\n",
        "    for x in content.strings:\n",
        "        x=x.strip()\n",
        "        if x:\n",
        "            stop_words.append(x)\n",
        "    print(f\"# Korean stop words: {len(stop_words)}\")\n",
        "else:\n",
        "    print(response.status_code)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:27:15.642775Z",
          "start_time": "2022-02-19T14:27:15.635333Z"
        },
        "id": "3d0IqhDF4WSk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "e5a13148-b3b2-4e05-e81b-7c8e2c7d2908"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'아'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "stop_words[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_t76Q1pQ4WSk"
      },
      "source": [
        "### 단어 사전 구축 함수 구현 \n",
        "- 문서 리스트를 입력 받아 사전을 생성하는 `make_vocab()` 함수를 구현하라.\n",
        "- 함수 정의\n",
        "    - 입력 매개변수\n",
        "        - docs : 문서 리스트\n",
        "        - min_count : 최소 단어 등장 빈도수 (단어 빈도가 `min_count` 미만인 단어는 사전에 포함하지 않음)\n",
        "    - 조건\n",
        "        - 문서 길이 제한\n",
        "            - 단어 개수가 3개 이하인 문서는 처리하지 않음. (skip)\n",
        "        - 사전에 포함되는 단어 빈도수 제한\n",
        "            - 단어가 빈도가 `min_count` 미만은 단어는 사전에 포함하지 않음.\n",
        "        - 불용어 제거 \n",
        "            - 불용어 리스트에 포함된 단어는 제거 \n",
        "    - 반환값 \n",
        "        - word2count : 단어별 빈도 사전 (key: 단어, value: 등장 횟수)\n",
        "        - wid2word : 단어별 인덱스(wid) 사전 (key: 단어 인덱스(int), value: 단어)\n",
        "        - word2wid : 인덱스(wid)별 단어 사전 (key: 단어, value: 단어 인덱스(int))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:33:01.637431Z",
          "start_time": "2022-02-19T14:32:56.730711Z"
        },
        "id": "xkjqztIA4WSl"
      },
      "outputs": [],
      "source": [
        "# 코퍼스 로드  \n",
        "# pd.read_csv()로 한 경우, random.sample(\"df\") << df는 sequece가 아니므로 작동 안됨 >> open > readlines()(파일내용 전체 리스트 반환)\n",
        "#docs = pd.read_csv('/content/drive/MyDrive/tokenized/wiki_ko_mecab.txt', sep= '\\t', header = None)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/drive/MyDrive/tokenized/wiki_ko_mecab.txt', 'r') as f:\n",
        "    list_file = f.readlines()\n",
        "docs = [line.rstrip('\\n') for line in list_file]"
      ],
      "metadata": {
        "id": "c4o3KA38e_Ur"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#docs[0]"
      ],
      "metadata": {
        "id": "7xrWV277oB0s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:33:03.423002Z",
          "start_time": "2022-02-19T14:33:03.419818Z"
        },
        "id": "WAKB6bbt4WSl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1bfc78a1-cc77-48ac-a25b-dae64a2ec3bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# wiki documents: 311,237\n"
          ]
        }
      ],
      "source": [
        "print(f\"# wiki documents: {len(docs):,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:33:04.016885Z",
          "start_time": "2022-02-19T14:33:03.962269Z"
        },
        "id": "-OI1MCXv4WSl"
      },
      "outputs": [],
      "source": [
        "# 문서 개수를 500개로 줄임\n",
        "docs=random.sample(docs, 500)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"# wiki documents: {len(docs):,}\")"
      ],
      "metadata": {
        "id": "mP5wGu9YwDUw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1070b485-8241-4abb-e309-a534198384cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# wiki documents: 500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#docs[499]"
      ],
      "metadata": {
        "id": "oYeL7RAwmMzW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "''' 한글 코드 >> [^ ㄱ-ㅣ가-힣]+\n",
        "## ㄱ ~ ㅎ: 0x3131 ~ 0x314e\n",
        "ㅏ ~ ㅣ: 0x314f ~ 0x3163\n",
        "가 ~ 힣: 0xac00 ~ 0xd7a3\n",
        "'''"
      ],
      "metadata": {
        "id": "ObTBnzKsKHDC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "e8db4c47-0b9f-4fdf-e84e-6fa8b9d4ac02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' 한글 코드 >> [^ ㄱ-ㅣ가-힣]+\\n## ㄱ ~ ㅎ: 0x3131 ~ 0x314e\\nㅏ ~ ㅣ: 0x314f ~ 0x3163\\n가 ~ 힣: 0xac00 ~ 0xd7a3\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#예시\n",
        "#a =  docs[499]\n",
        "#re.sub('[-=+,#/\\?:^$.@*\\\"※~&%ㆍ!』\\\\‘|\\(\\)\\[\\]\\<\\>`\\'…》A-Za-z0-9一-龥]',' ', a)"
      ],
      "metadata": {
        "id": "2Pa9Ja8QsYFl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:33:26.392627Z",
          "start_time": "2022-02-19T14:33:26.382358Z"
        },
        "id": "aJaEAVm9sAqv"
      },
      "outputs": [],
      "source": [
        "# 문서 내 숫자, 영어 대소문자, 특수문자를 제거 (re package 사용)\n",
        "for i in range(len(docs)):\n",
        "    docs[i] = re.sub('[-=+,#/\\?:^$.@*\\\"※~&%ㆍ!』/\\\\‘|\\(\\)\\[\\]\\<\\>《`\\'…》A-Za-z0-9一-龥]',' ', docs[i])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Check : {docs[0][:1000]}\")"
      ],
      "metadata": {
        "id": "sytiSICawMk5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "702e1671-0f28-4b68-f2ae-c78ce232a782"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Check : 남모 공주            는 신라 의 공주   왕족 으로 법흥왕 과 보과 공주 부여 씨 의 딸 이 며 백제 동성왕 의 외손녀 였 다   경쟁자 인 준정 과 함께 신라 의 초대 여성 원화   화랑   였 다   그 가 준정 에게 암살 당한 것 을 계기 로 화랑 은 여성 이 아닌 남성 미소년 으로 선발 하 게 되 었 다   신라 진흥왕 에게 는 사촌 누나 이 자 이모 가 된다   신라 의 청소년 조직 이 었 던 화랑도 는 처음 에 는 남모   준정 두 미녀 를 뽑 아 이 를   원화 라 했으며 이 들 주위 에 는       여 명 의 무리 를 따르 게 하 였 다   그러나 준정 과 남모 는 서로 최고 가 되 고자 시기 하 였 다   준정 은 박영실 을 섬겼 는데   지소태후 는 자신 의 두 번 째 남편 이 기 도 한 그 를 싫어해서 준정 의 원화 를 없애 고 낭도 가 부족 한 남모 에게 위화랑 의 낭도 를 더 해 주 었 다   그 뒤 남모 는 준정 의 초대 로 그 의 집 에 갔 다가 억지로 권하 는 술 을 받아마시 고 취한 뒤 준정 에 의해 강물 에 던져져 살해 되 었 다   이 일 이 발각 돼 준 정도 사형 에 처해지 고 나라 에서 는 귀족 출신 의 잘 생기 고 품행 이 곧 은 남자 를 뽑 아 곱 게 단장 한 후 이 를 화랑 이 라 칭하 고 받들 게 하 였 다     부왕 신라 제     대 국왕 법흥왕 모후 보과 공주 부여 씨                   공주 남모 공주 외조부 백제 제     대 국왕 동성왕 외조모 신라 이찬 비지 의 딸   화랑전사 마루                  년   배우   박효빈   신라 법흥왕 백제 동성왕 준정 화랑 분류         년 죽음 분류   신라 의 왕녀 분류   신라 의 왕족 분류   화랑 분류   암살 된 사람 분류   독살 된 사람 분류   법흥왕\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:33:27.904880Z",
          "start_time": "2022-02-19T14:33:27.899620Z"
        },
        "id": "OAkkQsvO4WSl"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "def make_vocab(docs:List[str], min_count:int):\n",
        "    \"\"\"\n",
        "    'docs'문서 리스트를 입력 받아 단어 사전을 생성.\n",
        "    \n",
        "    return \n",
        "        - word2count : 단어별 빈도 사전\n",
        "        - wid2word : 단어별 인덱스(wid) 사전 \n",
        "        - word2wid : 인덱스(wid)별 단어 사전\n",
        "    \"\"\"\n",
        "\n",
        "    word2count = dict()\n",
        "    word2id = dict()\n",
        "    id2word = dict()\n",
        "\n",
        "    #tqdm: 작업진행률 확인\n",
        "\n",
        "        # 1. 문서 길이 제한 :단어 개수가 3개 이하인 문서는 처리하지 않음. (skip)\n",
        "    doc_list = []\n",
        "    for doc in tqdm(docs):\n",
        "        if len(doc) >=3:\n",
        "            doc_list.extend(doc.split())\n",
        "\n",
        "        # 2. 임시 딕셔너리(_word2count)에 단어별 등장 빈도 기록 :단어가 빈도가 min_count 미만은 단어는 사전에 포함하지 않음\n",
        "    doc_counts = Counter(doc_list)\n",
        "\n",
        "    _word2count = dict()\n",
        "    for key, val in doc_counts.items(): \n",
        "        if int(val) >= min_count:\n",
        "            _word2count[key] = val \n",
        "  \n",
        "\n",
        "    # 3. 불용어 제거: 불용어 리스트에 포함된 단어는 제거\n",
        "    for key in list(_word2count.keys()):\n",
        "        if key in stop_words:\n",
        "            _word2count.pop(key)\n",
        "\n",
        "\n",
        "        # 4. 토큰 최소 빈도를 만족하는 토큰만 사전에 추가\n",
        "        word2count = _word2count\n",
        "        for i, key in enumerate(word2count):\n",
        "            word2id[key] = i\n",
        "            id2word[i] = key \n",
        "\n",
        "    \n",
        "    return word2count, word2id, id2word"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:33:30.157872Z",
          "start_time": "2022-02-19T14:33:28.473330Z"
        },
        "id": "ieS5SiQx4WSm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dec1d60f-884e-448f-f6a1-f4a4e1a54067"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 500/500 [00:00<00:00, 16591.00it/s]\n"
          ]
        }
      ],
      "source": [
        "word2count, word2id, id2word = make_vocab(docs, min_count=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:33:30.754722Z",
          "start_time": "2022-02-19T14:33:30.752115Z"
        },
        "id": "cT1MRN1EJtx6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b420bbc-9d80-4b09-b5c3-7413d1f8416c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "163,541\n"
          ]
        }
      ],
      "source": [
        "doc_len = sum(word2count.values()) # 문서 내 모든 단어의 개수 (단어별 등장 빈도의 총 합)\n",
        "print(f\"{doc_len:,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:33:32.916830Z",
          "start_time": "2022-02-19T14:33:32.914355Z"
        },
        "id": "e_1MneB54WSm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27d35b74-9c40-497e-d3b1-a34cefc1805b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# unique word : 5,880\n"
          ]
        }
      ],
      "source": [
        "print(f\"# unique word : {len(word2id):,}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#id2word"
      ],
      "metadata": {
        "id": "lK_-UzusJ0vc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc_list = [doc.split() for doc in docs]\n",
        "#doc_list\n",
        "# #사전에 존재하는 단어 리스트\n",
        "word_lists = []\n",
        "for doc in doc_list:\n",
        "     word_in = []\n",
        "     for word in doc:\n",
        "         key = word2id.get(word, None)\n",
        "         if key != None:\n",
        "             word_in.append(key)\n",
        "     word_lists.append(word_in)\n",
        "print(word_lists[4])"
      ],
      "metadata": {
        "id": "nCRvK_lsJShz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77d1b7db-ce26-4ac1-e5d6-beb881023428"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[167, 20, 4, 5, 167, 8, 11, 96, 168, 169, 5, 35, 150, 23, 54, 92, 11, 168, 169, 5, 170, 168, 171, 169, 5, 172, 5, 173, 173, 174, 169, 175, 173, 174, 5, 176, 20, 176, 20, 177, 5, 20, 178, 179, 169, 20, 178, 179, 5, 180, 181, 181, 182, 168, 181, 182, 169, 180, 181, 181, 182, 5, 183, 181, 33, 184, 5, 185, 186, 176, 20, 169, 176, 20, 5, 187, 20, 99, 169, 187, 20, 99, 5, 169, 5, 181, 36, 169, 181, 36, 5, 188, 188, 175, 189, 190, 191, 143, 5, 191, 175, 191, 169, 191, 5, 192, 5, 169, 192, 191, 5, 5, 191, 5, 193, 5, 5, 5, 194, 175, 191, 5, 177, 5, 186, 195, 196, 169, 186, 195, 196, 5, 5, 197, 169, 197, 5, 169, 5, 198, 5, 199, 179, 20, 200, 5, 169, 187, 20, 201, 5, 5, 175, 202, 20, 203, 175, 204, 205, 5, 5, 206, 207, 198, 5, 206, 207, 208, 194, 175, 209, 180, 5, 210, 5, 211, 212, 5, 213, 17, 181, 214, 191, 5, 191, 5, 5, 17, 175, 5, 215, 191, 5, 216, 5, 176, 20, 5, 217, 175, 172, 5, 208, 194, 175, 5, 5, 218, 8, 5, 199, 205, 5, 5, 219, 220, 198, 5, 221, 214, 222, 198, 5, 223, 139, 133, 5, 224, 5, 225, 221, 5, 169, 5, 199, 179, 20, 200, 226, 5, 169, 5, 227, 5, 23, 228, 181, 182, 169, 23, 228, 181, 182, 5, 229, 229, 230, 192, 231, 175, 143, 5, 232, 5, 219, 204, 205, 5, 5, 191, 5, 198, 5, 233, 233, 193, 5, 234, 234, 8, 191, 169, 235, 236, 5, 234, 234, 8, 191, 5, 5, 20, 51, 194, 198, 5, 237, 238, 237, 5, 239, 240, 198, 169, 239, 240, 198, 5, 199, 241, 175, 175, 175, 20, 51, 242, 174, 169, 20, 51, 242, 174, 5, 229, 191, 5, 243, 244, 175, 245, 198, 5, 5, 246, 229, 247, 169, 247, 5, 221, 248, 99, 169, 221, 248, 99, 5, 175, 143, 168, 143, 5, 36, 168, 249, 198, 169, 249, 198, 5, 250, 36, 169, 250, 36, 5, 36, 169, 36, 5, 251, 20, 191, 203, 252, 175, 191, 251, 20, 191, 4, 5, 167, 4, 5, 167, 253, 4, 5, 167, 4, 5, 167, 4, 5, 167, 4, 5, 167, 5]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(word_lists)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bQPPwQ2MWGY-",
        "outputId": "5f1eff38-d365-4d9b-d208-dd6460650d55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "500"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# window_size = 2\n",
        "# pair_list = []\n",
        "# for words in word_lists[:2]:\n",
        "#     list_pair= []\n",
        "#     for i, idx in enumerate(words):\n",
        "#         if i-window_size >= 0 and i+ window_size < len(words):\n",
        "#             neighbor = words[i-window_size:i] + words[i+1:i+1+window_size]\n",
        "#             target = words[i]\n",
        "#             list_pair.append((target, neighbor))\n",
        "#     pair_list.append(list_pair)\n",
        "        \n",
        "# print(pair_list)"
      ],
      "metadata": {
        "id": "oP5ajnH7MZK9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#pair_list[0]"
      ],
      "metadata": {
        "id": "S5SvwLhJSAYS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pair = []\n",
        "# for pair in pair_list:\n",
        "#     tuple_list = []\n",
        "#     for i in range(len(pair)):\n",
        "#         target = pair[i][0]\n",
        "#         for j in range(window_size*2):\n",
        "#             neighbor = pair[i][1][j]\n",
        "#             #val = tuple([target, neighbor])\n",
        "#             tuple_list.append(tuple([target, neighbor]))\n",
        "# print(tuple_list)"
      ],
      "metadata": {
        "id": "gxdIJwR0RDuW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHxtZqtk4WSm"
      },
      "source": [
        "### Dataset 클래스 구현\n",
        "- Skip-Gram 방식의 학습 데이터 셋(`Tuple(target_word, context_word)`)을 생성하는 `CustomDataset` 클래스를 구현하라.\n",
        "- 클래스 정의\n",
        "    - 생성자(`__init__()` 함수) 입력 매개변수\n",
        "        - docs: 문서 리스트\n",
        "        - word2id: 단어별 인덱스(wid) 사전\n",
        "        - window_size: Skip-Gram의 윈도우 사이즈\n",
        "    - 메소드\n",
        "        - `make_pair()`\n",
        "            - 문서를 단어로 쪼개고, 사전에 존재하는 단어들만 단어 인덱스로 변경\n",
        "            - Skip-gram 방식의 `(target_word, context_word)` 페어(tuple)들을 `pairs` 리스트에 담아 반환\n",
        "        - `__len__()`\n",
        "            - `pairs` 리스트의 개수 반환\n",
        "        - `__getitem__(index)`\n",
        "            - `pairs` 리스트를 인덱싱\n",
        "    - 주의 사항\n",
        "        - `nn.Module`를 부모 클래스로 상속 받음 \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:33:38.111290Z",
          "start_time": "2022-02-19T14:33:38.104531Z"
        },
        "id": "UPiLcYCZ4WSm"
      },
      "outputs": [],
      "source": [
        "class CustomDataset(Dataset):\n",
        "    \"\"\"\n",
        "    문서 리스트를 받아 skip-gram 방식의 (target_word, context_word) 데이터 셋을 생성\n",
        "    \"\"\"\n",
        "    def __init__(self, docs:List[str], word2id:Dict[str,int], window_size:int=5):\n",
        "        self.docs = docs\n",
        "        self.word2id = word2id\n",
        "        self.window_size = window_size\n",
        "        self.pairs = self.make_pair()\n",
        "\n",
        "    \n",
        "    def make_pair(self):\n",
        "        \"\"\"\n",
        "        (target, context) 형식의 Skip-gram pair 데이터 셋 생성 \n",
        "        \"\"\"\n",
        "        #문서를 단어로 쪼갠 것\n",
        "        doc_list = [doc.split() for doc in docs]\n",
        "        #사전에 존재하는 단어 리스트\n",
        "        word_lists = []\n",
        "        for doc in doc_list:\n",
        "            word_in = []\n",
        "            for word in doc:\n",
        "                key = word2id.get(word, None)\n",
        "                if key != None:\n",
        "                    word_in.append(key)\n",
        "            word_lists.append(word_in)\n",
        "        \n",
        "        ###################################################################################오류 ##################수정 필요############\n",
        "        #skipgram 방식의 pair_list =[target, (neighbor)]\n",
        "        pair_list = []\n",
        "        for words in word_lists:\n",
        "            list_pair= []\n",
        "            for i, idx in enumerate(words):\n",
        "                if i-1-self.window_size >= 0 and i-1+ self.window_size < len(words):\n",
        "                    neighbor = words[i-1-self.window_size:i-1] + words[i-1:i-1+self.window_size]\n",
        "                    target = words[i-1]\n",
        "                    list_pair.append((target, neighbor))\n",
        "            pair_list.append(list_pair)               \n",
        "        #print(pair_list)\n",
        "\n",
        "        #pairs = (target, neighbor)\n",
        "        pairs = []\n",
        "        for pair in pair_list:\n",
        "            pair_unit = []\n",
        "            for i in range(len(pair)):\n",
        "                target = pair[i][0]\n",
        "                for j in range(self.window_size*2):\n",
        "                    neighbor = pair[i][1][j]\n",
        "                    pair_unit.append(tuple([target, neighbor]))\n",
        "            pairs.extend(pair_unit)    ## 문제에서 요구하는건 dateset 통으로 >> append 아닌 extend\n",
        "        #print(pairs)\n",
        "        return pairs\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.pairs)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        return self.pairs[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:33:38.945361Z",
          "start_time": "2022-02-19T14:33:38.385577Z"
        },
        "id": "YntOw2q94WSm"
      },
      "outputs": [],
      "source": [
        "dataset = CustomDataset(docs, word2id, window_size=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:33:38.949614Z",
          "start_time": "2022-02-19T14:33:38.946663Z"
        },
        "id": "-RpNbAjk4WSn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74cf92c9-2331-4d35-abf9-39bc3b509575"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2256850"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ],
      "source": [
        "len(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:33:43.072635Z",
          "start_time": "2022-02-19T14:33:43.069526Z"
        },
        "id": "1FBwcL4H4WSn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a533771e-b5b5-45a4-84e7-57e0357d107c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 0)"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ],
      "source": [
        "dataset[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:33:51.040595Z",
          "start_time": "2022-02-19T14:33:51.031473Z"
        },
        "id": "wTAwTjKk4WSn",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7029599b-9951-4b94-bd2e-436ecd852190"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(공주, 남모)\n",
            "(공주, 공주)\n",
            "(공주, 는)\n",
            "(공주, 신라)\n",
            "(공주, 왕족)\n",
            "(공주, 공주)\n",
            "(공주, 왕족)\n",
            "(공주, 부여)\n",
            "(공주, 부여)\n",
            "(공주, 공주)\n",
            "(왕족, 공주)\n",
            "(왕족, 는)\n",
            "(왕족, 신라)\n",
            "(왕족, 왕족)\n",
            "(왕족, 공주)\n",
            "(왕족, 왕족)\n",
            "(왕족, 부여)\n",
            "(왕족, 부여)\n",
            "(왕족, 공주)\n",
            "(왕족, 부여)\n",
            "(부여, 는)\n",
            "(부여, 신라)\n",
            "(부여, 왕족)\n",
            "(부여, 공주)\n",
            "(부여, 왕족)\n",
            "(부여, 부여)\n",
            "(부여, 부여)\n",
            "(부여, 공주)\n",
            "(부여, 부여)\n",
            "(부여, 씨)\n",
            "(부여, 신라)\n",
            "(부여, 왕족)\n",
            "(부여, 공주)\n",
            "(부여, 왕족)\n",
            "(부여, 부여)\n",
            "(부여, 부여)\n",
            "(부여, 공주)\n",
            "(부여, 부여)\n",
            "(부여, 씨)\n",
            "(부여, 왕족)\n",
            "(공주, 왕족)\n",
            "(공주, 공주)\n",
            "(공주, 왕족)\n",
            "(공주, 부여)\n",
            "(공주, 부여)\n",
            "(공주, 공주)\n",
            "(공주, 부여)\n",
            "(공주, 씨)\n",
            "(공주, 왕족)\n",
            "(공주, 딸)\n",
            "(부여, 공주)\n",
            "(부여, 왕족)\n",
            "(부여, 부여)\n",
            "(부여, 부여)\n",
            "(부여, 공주)\n",
            "(부여, 부여)\n",
            "(부여, 씨)\n",
            "(부여, 왕족)\n",
            "(부여, 딸)\n",
            "(부여, 며)\n",
            "(씨, 왕족)\n",
            "(씨, 부여)\n",
            "(씨, 부여)\n",
            "(씨, 공주)\n",
            "(씨, 부여)\n",
            "(씨, 씨)\n",
            "(씨, 왕족)\n",
            "(씨, 딸)\n",
            "(씨, 며)\n",
            "(씨, 며)\n",
            "(왕족, 부여)\n",
            "(왕족, 부여)\n",
            "(왕족, 공주)\n",
            "(왕족, 부여)\n",
            "(왕족, 씨)\n",
            "(왕족, 왕족)\n",
            "(왕족, 딸)\n",
            "(왕족, 며)\n",
            "(왕족, 며)\n",
            "(왕족, 백제)\n",
            "(딸, 부여)\n",
            "(딸, 공주)\n",
            "(딸, 부여)\n",
            "(딸, 씨)\n",
            "(딸, 왕족)\n",
            "(딸, 딸)\n",
            "(딸, 며)\n",
            "(딸, 며)\n",
            "(딸, 백제)\n",
            "(딸, 왕족)\n",
            "(며, 공주)\n",
            "(며, 부여)\n",
            "(며, 씨)\n",
            "(며, 왕족)\n",
            "(며, 딸)\n",
            "(며, 며)\n",
            "(며, 며)\n",
            "(며, 백제)\n",
            "(며, 왕족)\n",
            "(며, 였)\n"
          ]
        }
      ],
      "source": [
        "# verify (target word, context word)\n",
        "for i, pair in enumerate(dataset):\n",
        "    if i==100:\n",
        "        break\n",
        "    print(f\"({id2word[pair[0]]}, {id2word[pair[1]]})\")\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0Z50-Dr4WSn"
      },
      "source": [
        "### 위에서 생성한 `dataset`으로 DataLoader  객체 생성\n",
        "- `DataLoader` 클래스로 `train_dataloader`객체를 생성하라. \n",
        "    - 생성자 매개변수와 값\n",
        "        - dataset = 위에서 생성한 dataset\n",
        "        - batch_size = 64\n",
        "        - shuffle = True"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = CustomDataset(docs, word2id, window_size=5)"
      ],
      "metadata": {
        "id": "R2vLQ_ymb9O0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:34:02.645176Z",
          "start_time": "2022-02-19T14:34:02.642780Z"
        },
        "id": "GXcAvFB14WSn"
      },
      "outputs": [],
      "source": [
        "train_dataloader = DataLoader(dataset, \n",
        "                              batch_size = 64,\n",
        "                              shuffle = True\n",
        "\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:34:02.777322Z",
          "start_time": "2022-02-19T14:34:02.774335Z"
        },
        "id": "4Yfcwi_14WSn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8988cc50-0d63-47aa-d87f-ced92c7d0874"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "35264"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ],
      "source": [
        "len(train_dataloader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eTs16gsU4WSn"
      },
      "source": [
        "### Negative Sampling 함수 구현\n",
        "- Skip-Gram은 복잡도를 줄이기 위한 방법으로 negative sampling을 사용한다. \n",
        "- `sample_table`이 다음과 같이 주어졌을 때, sample_table에서 랜덤으로 값을 뽑아 (batch_size, n_neg_sample) shape의 matrix를 반환하는 `get_neg_v_negative_sampling()`함수를 구현하라. \n",
        "- Sample Table은 negative distribution을 따른다. \n",
        "    - [negative distribution 설명](https://aegis4048.github.io/optimize_computational_efficiency_of_skip-gram_with_negative_sampling#How-are-negative-samples-drawn?)\n",
        "- 함수 정의\n",
        "    - 입력 매개변수\n",
        "        - batch_size : 배치 사이즈, matrix의 row 개수 \n",
        "        - n_neg_sample : negative sample의 개수, matrix의 column 개수\n",
        "    - 반환값 \n",
        "        - neg_v : 추출된 negative sample (2차원의 리스트)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:34:11.397509Z",
          "start_time": "2022-02-19T14:34:11.386389Z"
        },
        "id": "PUqIB6dH4WSn",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# negative sample을 추출할 sample table 생성 (해당 코드를 참고)\n",
        "sample_table = []\n",
        "sample_table_size = doc_len  #단어별 등장빈도 총합\n",
        "\n",
        "# noise distribution 생성\n",
        "alpha = 3/4\n",
        "frequency_list = np.array(list(word2count.values())) ** alpha\n",
        "Z = sum(frequency_list)\n",
        "ratio = frequency_list/Z\n",
        "negative_sample_dist = np.round(ratio*sample_table_size)\n",
        "\n",
        "for wid, c in enumerate(negative_sample_dist):\n",
        "    sample_table.extend([wid]*int(c))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:34:11.508414Z",
          "start_time": "2022-02-19T14:34:11.505464Z"
        },
        "id": "Wdu8qK8x4WSn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c71741c6-8299-4513-e634-a1ef7fbbf3fa"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "163355"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ],
      "source": [
        "len(sample_table)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 4\n",
        "n_neg_sample = 5\n",
        "q = np.random.choice(sample_table, (batch_size, n_neg_sample))\n",
        "q"
      ],
      "metadata": {
        "id": "qmfpsOlxf5zx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4942c0c-7974-4dc3-8f00-d3b6abdd788b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[2438,   20, 2201,  948, 1054],\n",
              "       [ 344, 5697,   20,  797,  411],\n",
              "       [ 831,  703, 1615, 2085, 2712],\n",
              "       [3530, 2653, 2123,   23,  433]])"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:34:11.656046Z",
          "start_time": "2022-02-19T14:34:11.653325Z"
        },
        "id": "mQIVrOIR4WSn"
      },
      "outputs": [],
      "source": [
        "def get_neg_v_negative_sampling(batch_size:int, n_neg_sample:int):\n",
        "    \"\"\"\n",
        "    위에서 정의한 sample_table에서 (batch_size, n_neg_sample) shape만큼 랜덤 추출해 \"네거티브 샘플 메트릭스\"를 생성\n",
        "    np.random.choice() 함수 활용 (위에서 정의한 sample_table을 함수의 argument로 사용)\n",
        "    \"\"\"\n",
        "    neg_v = np.random.choice(sample_table, (batch_size, n_neg_sample))\n",
        "    \n",
        "    return neg_v"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:34:12.345976Z",
          "start_time": "2022-02-19T14:34:12.333448Z"
        },
        "id": "8wwT4Af04WSo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "abd0cc56-d376-45fd-9648-c370742ac904"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  95, 1522, 3943,   51, 5365],\n",
              "       [  91, 2434,  490,    2,  587],\n",
              "       [ 729,   83, 2090,  862,  451],\n",
              "       [4581, 2827,   57, 1606, 1379]])"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ],
      "source": [
        "get_neg_v_negative_sampling(4, 5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nLnDXPvJ4WSo"
      },
      "source": [
        "## Challenge"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B5UubCzK4WSo"
      },
      "source": [
        "### 미니 튜토리얼\n",
        "- 아래 튜토리얼을 따라하며 Skip-Gram 모델의 `forward` 및 `loss` 연산 방식을 이해하자\n",
        "- Reference\n",
        "    - [torch.nn.Embedding](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html)\n",
        "    - [torch bmm](https://pytorch.org/docs/stable/generated/torch.bmm.html)\n",
        "    - [Skip-Gram negative sampling loss function 설명 영문 블로그](https://aegis4048.github.io/optimize_computational_efficiency_of_skip-gram_with_negative_sampling#Derivation-of-Cost-Function-in-Negative-Sampling)\n",
        "    - [Skip-Gram negative sampling loss function 설명 한글 블로그](https://reniew.github.io/22/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T12:51:46.954048Z",
          "start_time": "2022-02-19T12:51:46.951529Z"
        },
        "id": "IAR68hsY4WSo"
      },
      "outputs": [],
      "source": [
        "# hyper parameter example\n",
        "emb_size = 30000 # vocab size\n",
        "emb_dimension = 300 # word embedding 차원\n",
        "n_neg_sample = 5\n",
        "batch_size = 32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T12:51:49.340056Z",
          "start_time": "2022-02-19T12:51:47.300999Z"
        },
        "id": "zzOsVUn94WSo"
      },
      "outputs": [],
      "source": [
        "# 1. Embedding Matrix와 Context Matrix를 생성\n",
        "u_embedding = nn.Embedding(emb_size, emb_dimension, sparse=True).to(device)\n",
        "v_embedding = nn.Embedding(emb_size, emb_dimension, sparse=True).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T12:51:49.352240Z",
          "start_time": "2022-02-19T12:51:49.341437Z"
        },
        "id": "I7J_ADc44WSo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8205651-e8cf-46ca-f13b-43412985a681"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Target word idx : tensor([24912, 17289, 12910,  2559, 12326, 27555, 23130, 27934, 15182, 24236,\n",
            "        24332,  8710, 24734, 18293,  5763,  7206, 21829, 21916,  7559, 25700,\n",
            "        17538,  9671, 10080,  9851, 18343, 29882, 27734, 15472,  9884, 27090,\n",
            "         7445, 29345]) Pos context word idx : tensor([ 2106, 24805,  3470, 27587,  1701,  1286, 24877, 14064, 15501,  3052,\n",
            "        26884, 16798, 20775, 16434, 28191, 23326, 11902,  7991,  7144, 15670,\n",
            "        23655,  5007,  5479,  1307, 28772, 18687, 21962, 12154, 20946, 25284,\n",
            "        22784, 18920]) Neg context word idx : [[2300  675 5064    2 3359]\n",
            " [ 641   83  755   83 4366]\n",
            " [ 591   54  144  506  705]\n",
            " [2583 2384 1930   24 4585]\n",
            " [  92  199  576 1144 3743]\n",
            " [2842 3912 1106   92 4156]\n",
            " [   2  413  576 1371 2000]\n",
            " [5399  810 5300 3999   11]\n",
            " [1317 3918 2245 1416   54]\n",
            " [5365 2123  752 3022  156]\n",
            " [1100    2 4972 5226 2196]\n",
            " [4504 2531 3152 2751 2155]\n",
            " [2223   54 1808   60 1321]\n",
            " [ 127 4396 5500 5524   92]\n",
            " [4085 4013 5524 2957 3798]\n",
            " [ 225 3875 1740 5522  866]\n",
            " [ 419 4112  754  675  914]\n",
            " [1348 3253 3144 1002 2630]\n",
            " [1099  440   84   11 3220]\n",
            " [2492  107  132 4093  341]\n",
            " [1999   20 3395  364 1253]\n",
            " [2539 1850  994 3948 2296]\n",
            " [4996  166   83 3272   95]\n",
            " [1725   11 4214    2  388]\n",
            " [2542 4435 2361 1418 3917]\n",
            " [  11   46   58  621 3875]\n",
            " [4937  345 1578 2101 1808]\n",
            " [3532 4601 2257 1840  171]\n",
            " [1206 1375 2878 4399 2056]\n",
            " [ 638  775 2440    0  782]\n",
            " [3846  820 1855 1013   92]\n",
            " [ 281 2560 2866   67  130]]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 2. wid(단어 인덱스)를 임의로 생성\n",
        "pos_u = torch.randint(high = emb_size, size = (batch_size,))\n",
        "pos_v = torch.randint(high = emb_size, size = (batch_size,))\n",
        "neg_v = get_neg_v_negative_sampling(batch_size, n_neg_sample)\n",
        "print(f\"Target word idx : {pos_u} Pos context word idx : {pos_v} Neg context word idx : {neg_v}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T12:51:49.364020Z",
          "start_time": "2022-02-19T12:51:49.353486Z"
        },
        "id": "4iEG0nCZ4WSo"
      },
      "outputs": [],
      "source": [
        "# 3. tensor로 변환\n",
        "pos_u = Variable(torch.LongTensor(pos_u)).to(device)\n",
        "pos_v = Variable(torch.LongTensor(pos_v)).to(device)\n",
        "neg_v = Variable(torch.LongTensor(neg_v)).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T12:51:51.391896Z",
          "start_time": "2022-02-19T12:51:51.387084Z"
        },
        "id": "gqbNbajG4WSo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97049003-1868-4858-b573-c46fb9e22f1c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape of pos_u embedding : torch.Size([32, 300])\n",
            " shape of pos_v embedding : torch.Size([32, 300])\n",
            " shape of neg_v embedding : torch.Size([32, 5, 300])\n"
          ]
        }
      ],
      "source": [
        "# 4. wid로 각각의 embedding matrix에서 word embedding 값을 가져오기\n",
        "pos_u = u_embedding(pos_u)\n",
        "pos_v = v_embedding(pos_v)\n",
        "neg_v = v_embedding(neg_v)\n",
        "print(f\"shape of pos_u embedding : {pos_u.shape}\\n shape of pos_v embedding : {pos_v.shape}\\n shape of neg_v embedding : {neg_v.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T12:51:53.121477Z",
          "start_time": "2022-02-19T12:51:52.646148Z"
        },
        "id": "uDWUrSwo4WSo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6164c9d-6e15-458f-daf1-4605d5dd47d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape of pos logits : torch.Size([32])\n",
            "\n",
            "shape of logits : torch.Size([32, 5])\n"
          ]
        }
      ],
      "source": [
        "# 5. dot product \n",
        "pos_score = torch.mul(pos_u, pos_v) # 행렬 element-wise 곱\n",
        "pos_score = torch.sum(pos_score, dim=1)\n",
        "print(f\"shape of pos logits : {pos_score.shape}\\n\")\n",
        "\n",
        "neg_score = torch.bmm(neg_v, pos_u.unsqueeze(dim=2)).squeeze()\n",
        "print(f\"shape of logits : {neg_score.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T12:51:53.670418Z",
          "start_time": "2022-02-19T12:51:53.665671Z"
        },
        "id": "adOpcoL54WSo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65f17d42-0fc7-4700-d99b-0dfd96b9e01f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pos logits : -216.42654418945312\n",
            "neg logits : -1058.96533203125\n",
            "Loss : 1275.391845703125\n"
          ]
        }
      ],
      "source": [
        "# 6. loss 구하기\n",
        "pos_score = F.logsigmoid(pos_score)\n",
        "neg_score = F.logsigmoid(-1*neg_score) # negative의 logit은 minimize 하기 위해 -1 곱함\n",
        "print(f\"pos logits : {pos_score.sum()}\")\n",
        "print(f\"neg logits : {neg_score.sum()}\")\n",
        "loss = -1 * (torch.sum(pos_score) + torch.sum(neg_score))\n",
        "print(f\"Loss : {loss}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "muEceOGZ4WSo"
      },
      "source": [
        "### Skip-gram 클래스 구현\n",
        "- Skip-Gram 방식으로 단어 embedding을 학습하는 `SkipGram` 클래스를 구현하라.\n",
        "- 클래스 정의\n",
        "    - 생성자(`__init__()` 함수) 입력 매개변수\n",
        "        - `vocab_size` : 사전내 단어 개수\n",
        "        - `emb_dimension` : 엠베딩 크기\n",
        "        - `device` : 연산 장치 종류\n",
        "    - 생성자에서 생성해야할 변수 \n",
        "        - `vocab_size` : 사전내 단어 개수\n",
        "        - `emb_dimension` : 엠베딩 크기\n",
        "        - `u_embedding` : (vocab_size, emb_dimension) 엠베딩 메트릭스 (target_word)\n",
        "        - `v_embedding` : (vocab_size, emb_dimension) 엠베딩 메트릭스 (context_word)\n",
        "    - 메소드\n",
        "        - `init_embedding()` (제공됨)\n",
        "            - 엠베딩 메트릭스 값을 초기화\n",
        "        - `forward()`\n",
        "            - 위 튜토리얼과 같이 dot product를 수행한 후 score를 생성\n",
        "            - loss를 반환 (loss 설명 추가)\n",
        "        - `save_emedding()` (제공됨)\n",
        "            - `u_embedding`의 단어 엠베딩 값을 단어 별로 파일에 저장\n",
        "    - 주의 사항     \n",
        "        - `nn.Module`를 부모 클래스로 상속 받음 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:34:15.731306Z",
          "start_time": "2022-02-19T14:34:15.721129Z"
        },
        "id": "pnmMamP44WSo"
      },
      "outputs": [],
      "source": [
        "class SkipGram(nn.Module):\n",
        "    def __init__(self, vocab_size:int, emb_dimension:int, device:str):\n",
        "        super(SkipGram, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.emb_dimension = emb_dimension\n",
        "        self.u_embedding = nn.Embedding(vocab_size, emb_dimension, sparse=True).to(device)\n",
        "        self.v_embedding = nn.Embedding(vocab_size, emb_dimension, sparse=True).to(device)\n",
        "        self.init_embedding()\n",
        "    \n",
        "    \n",
        "    def init_embedding(self):\n",
        "        \"\"\"\n",
        "        u_embedding과 v_embedding 메트릭스 값을 초기화\n",
        "        \"\"\"\n",
        "        initrange = 0.5 / self.emb_dimension\n",
        "        self.u_embedding.weight.data.uniform_(-initrange, initrange)\n",
        "        self.v_embedding.weight.data.uniform_(-0, 0)\n",
        "    \n",
        "    \n",
        "    def forward(self, pos_u, pos_v, neg_v):\n",
        "        \"\"\"\n",
        "        dot product를 수행한 후 score를 생성\n",
        "        loss 반환\n",
        "        \"\"\"    \n",
        "\n",
        "\n",
        "        # 각각의 embedding matrix에서 word embedding 값을 가져오기\n",
        "\n",
        "        # pos_u = Variable(torch.LongTensor(pos_u)).to(device)\n",
        "        # pos_v = Variable(torch.LongTensor(pos_v)).to(device)\n",
        "        # neg_v = Variable(torch.LongTensor(neg_v)).to(device)\n",
        "\n",
        "        pos_u = self.u_embedding(pos_u)\n",
        "        pos_v = self.v_embedding(pos_v)\n",
        "        neg_v = self.v_embedding(neg_v)\n",
        "\n",
        "        # dot product \n",
        "        pos_score = torch.mul(pos_u, pos_v)\n",
        "        pos_score = torch.sum(pos_score, dim=1)\n",
        "        neg_score = torch.bmm(neg_v, pos_u.unsqueeze(dim=2)).squeeze()\n",
        "        \n",
        "\n",
        "        # loss 구하기\n",
        "        pos_score = F.logsigmoid(pos_score)\n",
        "        neg_score = F.logsigmoid(-1*neg_score)\n",
        "        loss = -1 * (torch.sum(pos_score) + torch.sum(neg_score))\n",
        "\n",
        "        return loss\n",
        "    \n",
        "    def save_embedding(self, id2word, file_name, use_cuda):\n",
        "        \"\"\"\n",
        "        'file_name' 위치에 word와 word_embedding을 line-by로 저장\n",
        "        파일의 첫 줄은 '단어 개수' 그리고 '단어 embedding 사이즈' 값을 입력해야 함\n",
        "        \"\"\"\n",
        "        if use_cuda: # parameter를 gpu 메모리에서 cpu 메모리로 옮김\n",
        "            embedding = self.u_embedding.weight.cpu().data.numpy()\n",
        "        else:\n",
        "            embedding = self.u_embedding.weight.data.numpy()\n",
        "\n",
        "        with open(file_name, 'w') as writer:\n",
        "            # 파일의 첫 줄은 '단어 개수' 그리고 '단어 embedding 사이즈' 값을 입력해야 함\n",
        "            writer.write(f\"{len(id2word)} {embedding.shape[-1]}\\n\")\n",
        "            \n",
        "            for wid, word in id2word.items():\n",
        "                e = embedding[wid]\n",
        "                e = \" \".join([str(e_) for e_ in e])\n",
        "                writer.write(f\"{word} {e}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RqqMo0zL4WSo"
      },
      "source": [
        "## Advanced"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wSWd5gV24WSp"
      },
      "source": [
        "### Skip-Gram 방식의  Word2Vec 클래스 구현\n",
        "- Skip-Gram 방식으로 단어 embedding을 학습하는 `Word2Vec` 클래스를 구현하라.\n",
        "- 클래스 정의\n",
        "    - 생성자(`__init__()`) 입력 매개 변수\n",
        "        - `input_file` : 학습할 문서 리스트\n",
        "        - `output_file_name` : 학습된 word embedding을 저장할 파일 위치\n",
        "        - `device` : 연상 장치 종류\n",
        "        - `emb_dimension` : word embedding 차원\n",
        "        - `batch_size` : 학습 배치 사이즈\n",
        "        - `window_size` : skip-gram 윈도우 사이즈 (context word 개수를 결정)\n",
        "        - `n_neg_sample` : negative sample 개수\n",
        "        - `iteration` : 학습 반복 횟수\n",
        "        - `lr` : learning rate\n",
        "        - `min_count` : 사전에 추가될 단어의 최소 등장 빈도\n",
        "    - 생성자에서 생성해야 할 변수 \n",
        "        - `docs` : 학습할 문서 리스트\n",
        "        - `output_file_name` : 학습된 word embedding을 저장할 파일 위치\n",
        "        - `word2count`, `word2id`, `id2word` : 위에서 구현한 `make_vocab()` 함수의 반환 값\n",
        "        - `device` : 연산 장치 종류\n",
        "        - `emb_size` : vocab의 (unique한) 단어 종류 \n",
        "        - `emb_dimension` : word embedding 차원\n",
        "        - `batch_size` : 학습 배치 사이즈\n",
        "        - `window_size` : skip-gram 윈도우 사이즈 (context word 개수를 결정)\n",
        "        - `n_neg_sample` : negative sample 개수\n",
        "        - `iteration` : 학습 반복 횟수\n",
        "        - `lr` : learning rate\n",
        "        - `model` : `SkipGram` 클래스의 인스턴스\n",
        "        - `optimizer` : `SGD` 클래스의 인스턴스\n",
        "    - 메소드\n",
        "        - `train()`\n",
        "            - 입력 매개변수 \n",
        "                - `train_dataloader`\n",
        "            - Iteration 횟수만큼 input_file 학습 데이터를 학습한다. 매 epoch마다 for loop 돌면서 batch 단위 학습 데이터를 skip gram 모델에 학습함. 학습이 끝나면 word embedding을 output_file_name 파일에 저장.\n",
        "- Reference\n",
        "    - [Optimizer - SGD](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "id": "4I1aMWllKkyf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "646c7b94-2651-40ed-81cf-f36e0804f06e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# device type\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(f\"# available GPUs : {torch.cuda.device_count()}\")\n",
        "    print(f\"GPU name : {torch.cuda.get_device_name()}\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "print(device)"
      ],
      "metadata": {
        "id": "z3cMh60BXDt2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8db67df-519d-4c84-9752-fa76ba8b2a26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# available GPUs : 1\n",
            "GPU name : Tesla P100-PCIE-16GB\n",
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:34:20.503555Z",
          "start_time": "2022-02-19T14:34:20.491585Z"
        },
        "id": "Td-GQrqI4WSp"
      },
      "outputs": [],
      "source": [
        "class Word2Vec:\n",
        "    def __init__(self, \n",
        "                input_file: List[str],\n",
        "                output_file_name: str,\n",
        "                 device: str,\n",
        "                 emb_dimension=300,\n",
        "                 batch_size = 64,\n",
        "                 window_size=5,\n",
        "                 n_neg_sample = 5,\n",
        "                 iteration=1,\n",
        "                 lr = 0.02,\n",
        "                 min_count=5):\n",
        "        self.docs = input_file\n",
        "        self.output_file_name = output_file_name\n",
        "        self.word2count, self.word2id, self.id2word = make_vocab(input_file, min_count)\n",
        "        self.device = device\n",
        "        self.emb_size = len(self.word2id)\n",
        "        self.emb_dimension = emb_dimension\n",
        "        self.batch_size = batch_size\n",
        "        self.window_size = window_size\n",
        "        self.n_neg_sample = n_neg_sample\n",
        "        self.iteration = iteration\n",
        "        self.lr = lr\n",
        "        self.model = SkipGram(self.emb_size, emb_dimension, device)\n",
        "        self.optimizer = torch.optim.SGD(self.model.parameters(), lr) # torch.optim.SGD 클래스 사용\n",
        "\n",
        "        # train() 함수에서 만든 임베딩 결과 파일들을 저장할 폴더 생성 (os.makedirs 사용)\n",
        "        ############## 이거때문에 너무 해맸다!! ############################\n",
        "        if not os.path.exists(self.output_file_name):\n",
        "            os.makedirs(self.output_file_name) \n",
        "        #self.model.save_embedding(self, self.id2word, os.makedirs('/content/drive/MyDrive/processed/',exist_ok=True), use_cuda)   \n",
        "       \n",
        "        \n",
        "    \n",
        "    def train(self, train_dataloader):\n",
        "        \n",
        "        # lr 값을 조절하는 스케줄러 인스턴스 변수를 생성\n",
        "        self.scheduler = get_linear_schedule_with_warmup(\n",
        "            optimizer = self.optimizer,\n",
        "            num_warmup_steps=0,\n",
        "            num_training_steps= len(train_dataloader)\n",
        "        )\n",
        "        \n",
        "        for epoch in range(self.iteration):\n",
        "            \n",
        "            print(f\"*****Epoch {epoch} Train Start*****\")\n",
        "            print(f\"*****Epoch {epoch} Total Step {len(train_dataloader)}*****\")\n",
        "            total_loss, batch_loss, batch_step = 0,0,0\n",
        "\n",
        "            for step, batch in enumerate(train_dataloader):\n",
        "                batch_step+=1\n",
        "\n",
        "                pos_u, pos_v = batch\n",
        "                # negative data 생성\n",
        "                neg_v = get_neg_v_negative_sampling(pos_u.shape[0], self.n_neg_sample)\n",
        "                \n",
        "                # 데이터를 tensor화 & device 설정\n",
        "                pos_u = Variable(torch.LongTensor(pos_u)).to(device)\n",
        "                pos_v = Variable(torch.LongTensor(pos_v)).to(device)\n",
        "                neg_v = Variable(torch.LongTensor(neg_v)).to(device)\n",
        "\n",
        "                # model의 gradient 초기화\n",
        "                self.model.zero_grad()\n",
        "                # optimizer의 gradient 초기화\n",
        "                self.optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                loss = self.model.forward(pos_u, pos_v, neg_v)\n",
        "\n",
        "                # loss 계산\n",
        "                loss.backward()\n",
        "\n",
        "                # optimizer 업데이트\n",
        "                self.optimizer.step() \n",
        "                # scheduler 업데이트\n",
        "                self.scheduler.step()\n",
        "\n",
        "                batch_loss += loss.item()\n",
        "                total_loss += loss.item()\n",
        "                \n",
        "                if (step%500 == 0) and (step!=0):\n",
        "                    print(f\"Step: {step} Loss: {batch_loss/batch_step:.4f} lr: {self.optimizer.param_groups[0]['lr']:.4f}\")\n",
        "                    # 변수 초기화    \n",
        "                    batch_loss, batch_step = 0,0\n",
        "            \n",
        "            print(f\"Epoch {epoch} Total Mean Loss : {total_loss/(step+1):.4f}\")\n",
        "            print(f\"*****Epoch {epoch} Train Finished*****\\n\")\n",
        "            \n",
        "            print(f\"*****Epoch {epoch} Saving Embedding...*****\")\n",
        "            self.model.save_embedding(self.id2word, os.path.join(self.output_file_name, f'w2v_{epoch}.txt'), True if 'cuda' in self.device.type else False)\n",
        "            print(f\"*****Epoch {epoch} Embedding Saved at {os.path.join(self.output_file_name, f'w2v_{epoch}.txt')}*****\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:34:29.561892Z",
          "start_time": "2022-02-19T14:34:26.103659Z"
        },
        "id": "Ywx9R8n24WSp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce0f9602-378b-479c-e2ff-d657fc2a5c40"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 500/500 [00:00<00:00, 20704.64it/s]\n"
          ]
        }
      ],
      "source": [
        "output_file = os.path.join(\".\", \"word2vec_wiki\")\n",
        "# Word2Vec 클래스의 인스턴스 생성\n",
        "w2v = Word2Vec(docs, output_file, device, n_neg_sample=10, iteration=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:34:34.615469Z",
          "start_time": "2022-02-19T14:34:34.055502Z"
        },
        "id": "ufBxjKxN4WSp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a39ee202-8dd1-4083-c6c1-345bf330a167"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "35264"
            ]
          },
          "metadata": {},
          "execution_count": 95
        }
      ],
      "source": [
        "# 학습 데이터 셋 및 데이터 로더 생성 (위에서 생성한 w2v의 attribute들을 argument에 적절히 넣기)\n",
        "dataset = CustomDataset(w2v.docs, w2v.word2id, w2v.window_size)\n",
        "train_dataloader = DataLoader(dataset, \n",
        "                              batch_size = w2v.batch_size,\n",
        "                              shuffle = True\n",
        "\n",
        ")\n",
        "len(train_dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:45:38.362817Z",
          "start_time": "2022-02-19T14:34:37.382371Z"
        },
        "id": "9JBUrUJ34WSp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1d75f37-47e8-4bc9-bc16-a9f45350d2c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*****Epoch 0 Train Start*****\n",
            "*****Epoch 0 Total Step 35264*****\n",
            "Step: 500 Loss: 484.2486 lr: 0.0197\n",
            "Step: 1000 Loss: 386.4143 lr: 0.0194\n",
            "Step: 1500 Loss: 312.6190 lr: 0.0191\n",
            "Step: 2000 Loss: 280.2149 lr: 0.0189\n",
            "Step: 2500 Loss: 256.5084 lr: 0.0186\n",
            "Step: 3000 Loss: 238.4600 lr: 0.0183\n",
            "Step: 3500 Loss: 225.3050 lr: 0.0180\n",
            "Step: 4000 Loss: 214.8932 lr: 0.0177\n",
            "Step: 4500 Loss: 207.3881 lr: 0.0174\n",
            "Step: 5000 Loss: 200.4794 lr: 0.0172\n",
            "Step: 5500 Loss: 196.9314 lr: 0.0169\n",
            "Step: 6000 Loss: 191.5448 lr: 0.0166\n",
            "Step: 6500 Loss: 189.0682 lr: 0.0163\n",
            "Step: 7000 Loss: 184.9562 lr: 0.0160\n",
            "Step: 7500 Loss: 182.5988 lr: 0.0157\n",
            "Step: 8000 Loss: 179.2890 lr: 0.0155\n",
            "Step: 8500 Loss: 178.5331 lr: 0.0152\n",
            "Step: 9000 Loss: 176.7237 lr: 0.0149\n",
            "Step: 9500 Loss: 175.3078 lr: 0.0146\n",
            "Step: 10000 Loss: 174.2316 lr: 0.0143\n",
            "Step: 10500 Loss: 173.1132 lr: 0.0140\n",
            "Step: 11000 Loss: 172.1300 lr: 0.0138\n",
            "Step: 11500 Loss: 170.5542 lr: 0.0135\n",
            "Step: 12000 Loss: 170.5382 lr: 0.0132\n",
            "Step: 12500 Loss: 168.9157 lr: 0.0129\n",
            "Step: 13000 Loss: 168.7044 lr: 0.0126\n",
            "Step: 13500 Loss: 168.1745 lr: 0.0123\n",
            "Step: 14000 Loss: 167.6816 lr: 0.0121\n",
            "Step: 14500 Loss: 166.3941 lr: 0.0118\n",
            "Step: 15000 Loss: 166.8768 lr: 0.0115\n",
            "Step: 15500 Loss: 166.0807 lr: 0.0112\n",
            "Step: 16000 Loss: 165.0545 lr: 0.0109\n",
            "Step: 16500 Loss: 164.7594 lr: 0.0106\n",
            "Step: 17000 Loss: 163.8463 lr: 0.0104\n",
            "Step: 17500 Loss: 163.8536 lr: 0.0101\n",
            "Step: 18000 Loss: 163.7035 lr: 0.0098\n",
            "Step: 18500 Loss: 163.3694 lr: 0.0095\n",
            "Step: 19000 Loss: 162.8040 lr: 0.0092\n",
            "Step: 19500 Loss: 163.2925 lr: 0.0089\n",
            "Step: 20000 Loss: 162.6522 lr: 0.0087\n",
            "Step: 20500 Loss: 162.6907 lr: 0.0084\n",
            "Step: 21000 Loss: 162.4638 lr: 0.0081\n",
            "Step: 21500 Loss: 162.4549 lr: 0.0078\n",
            "Step: 22000 Loss: 161.3364 lr: 0.0075\n",
            "Step: 22500 Loss: 162.0353 lr: 0.0072\n",
            "Step: 23000 Loss: 162.1703 lr: 0.0070\n",
            "Step: 23500 Loss: 160.9766 lr: 0.0067\n",
            "Step: 24000 Loss: 161.0511 lr: 0.0064\n",
            "Step: 24500 Loss: 161.0869 lr: 0.0061\n",
            "Step: 25000 Loss: 160.5074 lr: 0.0058\n",
            "Step: 25500 Loss: 160.1881 lr: 0.0055\n",
            "Step: 26000 Loss: 160.8248 lr: 0.0053\n",
            "Step: 26500 Loss: 160.3101 lr: 0.0050\n",
            "Step: 27000 Loss: 160.6988 lr: 0.0047\n",
            "Step: 27500 Loss: 160.1056 lr: 0.0044\n",
            "Step: 28000 Loss: 160.7472 lr: 0.0041\n",
            "Step: 28500 Loss: 160.5973 lr: 0.0038\n",
            "Step: 29000 Loss: 159.9210 lr: 0.0036\n",
            "Step: 29500 Loss: 160.2680 lr: 0.0033\n",
            "Step: 30000 Loss: 160.4548 lr: 0.0030\n",
            "Step: 30500 Loss: 159.5264 lr: 0.0027\n",
            "Step: 31000 Loss: 160.0008 lr: 0.0024\n",
            "Step: 31500 Loss: 160.1119 lr: 0.0021\n",
            "Step: 32000 Loss: 159.5491 lr: 0.0019\n",
            "Step: 32500 Loss: 159.4903 lr: 0.0016\n",
            "Step: 33000 Loss: 160.1503 lr: 0.0013\n",
            "Step: 33500 Loss: 159.1564 lr: 0.0010\n",
            "Step: 34000 Loss: 159.6520 lr: 0.0007\n",
            "Step: 34500 Loss: 159.8399 lr: 0.0004\n",
            "Step: 35000 Loss: 158.9042 lr: 0.0001\n",
            "Epoch 0 Total Mean Loss : 182.4838\n",
            "*****Epoch 0 Train Finished*****\n",
            "\n",
            "*****Epoch 0 Saving Embedding...*****\n",
            "*****Epoch 0 Embedding Saved at ./word2vec_wiki/w2v_0.txt*****\n",
            "\n",
            "*****Epoch 1 Train Start*****\n",
            "*****Epoch 1 Total Step 35264*****\n",
            "Step: 500 Loss: 157.7750 lr: 0.0000\n",
            "Step: 1000 Loss: 158.4251 lr: 0.0000\n",
            "Step: 1500 Loss: 158.6983 lr: 0.0000\n",
            "Step: 2000 Loss: 159.6968 lr: 0.0000\n",
            "Step: 2500 Loss: 158.4125 lr: 0.0000\n",
            "Step: 3000 Loss: 159.0068 lr: 0.0000\n",
            "Step: 3500 Loss: 158.8973 lr: 0.0000\n",
            "Step: 4000 Loss: 158.2660 lr: 0.0000\n",
            "Step: 4500 Loss: 158.4117 lr: 0.0000\n",
            "Step: 5000 Loss: 158.2424 lr: 0.0000\n",
            "Step: 5500 Loss: 158.8952 lr: 0.0000\n",
            "Step: 6000 Loss: 158.2987 lr: 0.0000\n",
            "Step: 6500 Loss: 158.3434 lr: 0.0000\n",
            "Step: 7000 Loss: 159.5506 lr: 0.0000\n",
            "Step: 7500 Loss: 159.0842 lr: 0.0000\n",
            "Step: 8000 Loss: 158.8305 lr: 0.0000\n",
            "Step: 8500 Loss: 158.9207 lr: 0.0000\n",
            "Step: 9000 Loss: 158.8800 lr: 0.0000\n",
            "Step: 9500 Loss: 158.5400 lr: 0.0000\n",
            "Step: 10000 Loss: 158.8689 lr: 0.0000\n",
            "Step: 10500 Loss: 159.0573 lr: 0.0000\n",
            "Step: 11000 Loss: 158.7231 lr: 0.0000\n",
            "Step: 11500 Loss: 158.9259 lr: 0.0000\n",
            "Step: 12000 Loss: 158.9756 lr: 0.0000\n",
            "Step: 12500 Loss: 158.6426 lr: 0.0000\n",
            "Step: 13000 Loss: 158.6992 lr: 0.0000\n",
            "Step: 13500 Loss: 158.4348 lr: 0.0000\n",
            "Step: 14000 Loss: 158.3503 lr: 0.0000\n",
            "Step: 14500 Loss: 158.3533 lr: 0.0000\n",
            "Step: 15000 Loss: 158.2123 lr: 0.0000\n",
            "Step: 15500 Loss: 159.3332 lr: 0.0000\n",
            "Step: 16000 Loss: 158.1102 lr: 0.0000\n",
            "Step: 16500 Loss: 158.6770 lr: 0.0000\n",
            "Step: 17000 Loss: 159.3982 lr: 0.0000\n",
            "Step: 17500 Loss: 159.1902 lr: 0.0000\n",
            "Step: 18000 Loss: 159.5067 lr: 0.0000\n",
            "Step: 18500 Loss: 159.3431 lr: 0.0000\n",
            "Step: 19000 Loss: 158.4488 lr: 0.0000\n",
            "Step: 19500 Loss: 158.3313 lr: 0.0000\n",
            "Step: 20000 Loss: 159.3349 lr: 0.0000\n",
            "Step: 20500 Loss: 159.2573 lr: 0.0000\n",
            "Step: 21000 Loss: 158.3561 lr: 0.0000\n",
            "Step: 21500 Loss: 159.1168 lr: 0.0000\n",
            "Step: 22000 Loss: 159.2395 lr: 0.0000\n",
            "Step: 22500 Loss: 159.1723 lr: 0.0000\n",
            "Step: 23000 Loss: 158.8257 lr: 0.0000\n",
            "Step: 23500 Loss: 158.0449 lr: 0.0000\n",
            "Step: 24000 Loss: 158.6858 lr: 0.0000\n",
            "Step: 24500 Loss: 158.9884 lr: 0.0000\n",
            "Step: 25000 Loss: 159.1966 lr: 0.0000\n",
            "Step: 25500 Loss: 158.9915 lr: 0.0000\n",
            "Step: 26000 Loss: 159.0310 lr: 0.0000\n",
            "Step: 26500 Loss: 158.6602 lr: 0.0000\n",
            "Step: 27000 Loss: 159.0055 lr: 0.0000\n",
            "Step: 27500 Loss: 158.3002 lr: 0.0000\n",
            "Step: 28000 Loss: 157.8296 lr: 0.0000\n",
            "Step: 28500 Loss: 159.6417 lr: 0.0000\n",
            "Step: 29000 Loss: 158.9632 lr: 0.0000\n",
            "Step: 29500 Loss: 159.1721 lr: 0.0000\n",
            "Step: 30000 Loss: 157.7991 lr: 0.0000\n",
            "Step: 30500 Loss: 158.8874 lr: 0.0000\n",
            "Step: 31000 Loss: 158.5769 lr: 0.0000\n",
            "Step: 31500 Loss: 159.2649 lr: 0.0000\n",
            "Step: 32000 Loss: 159.5077 lr: 0.0000\n",
            "Step: 32500 Loss: 158.5412 lr: 0.0000\n",
            "Step: 33000 Loss: 158.9018 lr: 0.0000\n",
            "Step: 33500 Loss: 158.9088 lr: 0.0000\n",
            "Step: 34000 Loss: 158.4653 lr: 0.0000\n",
            "Step: 34500 Loss: 158.3393 lr: 0.0000\n",
            "Step: 35000 Loss: 159.0514 lr: 0.0000\n",
            "Epoch 1 Total Mean Loss : 158.7772\n",
            "*****Epoch 1 Train Finished*****\n",
            "\n",
            "*****Epoch 1 Saving Embedding...*****\n",
            "*****Epoch 1 Embedding Saved at ./word2vec_wiki/w2v_1.txt*****\n",
            "\n",
            "*****Epoch 2 Train Start*****\n",
            "*****Epoch 2 Total Step 35264*****\n",
            "Step: 500 Loss: 158.3214 lr: 0.0000\n",
            "Step: 1000 Loss: 158.1207 lr: 0.0000\n",
            "Step: 1500 Loss: 159.1922 lr: 0.0000\n",
            "Step: 2000 Loss: 159.3362 lr: 0.0000\n",
            "Step: 2500 Loss: 158.6177 lr: 0.0000\n",
            "Step: 3000 Loss: 158.2296 lr: 0.0000\n",
            "Step: 3500 Loss: 157.9256 lr: 0.0000\n",
            "Step: 4000 Loss: 159.0255 lr: 0.0000\n",
            "Step: 4500 Loss: 158.6727 lr: 0.0000\n",
            "Step: 5000 Loss: 158.9550 lr: 0.0000\n",
            "Step: 5500 Loss: 159.5866 lr: 0.0000\n",
            "Step: 6000 Loss: 158.7320 lr: 0.0000\n",
            "Step: 6500 Loss: 159.0542 lr: 0.0000\n",
            "Step: 7000 Loss: 158.4532 lr: 0.0000\n",
            "Step: 7500 Loss: 158.9052 lr: 0.0000\n",
            "Step: 8000 Loss: 158.3177 lr: 0.0000\n",
            "Step: 8500 Loss: 158.7645 lr: 0.0000\n",
            "Step: 9000 Loss: 158.9675 lr: 0.0000\n",
            "Step: 9500 Loss: 159.0335 lr: 0.0000\n",
            "Step: 10000 Loss: 159.7608 lr: 0.0000\n",
            "Step: 10500 Loss: 158.3775 lr: 0.0000\n",
            "Step: 11000 Loss: 159.1757 lr: 0.0000\n",
            "Step: 11500 Loss: 158.0794 lr: 0.0000\n",
            "Step: 12000 Loss: 158.7253 lr: 0.0000\n",
            "Step: 12500 Loss: 159.0580 lr: 0.0000\n",
            "Step: 13000 Loss: 159.1187 lr: 0.0000\n",
            "Step: 13500 Loss: 158.6135 lr: 0.0000\n",
            "Step: 14000 Loss: 157.3723 lr: 0.0000\n",
            "Step: 14500 Loss: 158.4312 lr: 0.0000\n",
            "Step: 15000 Loss: 158.8797 lr: 0.0000\n",
            "Step: 15500 Loss: 159.2129 lr: 0.0000\n",
            "Step: 16000 Loss: 158.4083 lr: 0.0000\n",
            "Step: 16500 Loss: 158.4210 lr: 0.0000\n",
            "Step: 17000 Loss: 158.2959 lr: 0.0000\n",
            "Step: 17500 Loss: 158.3192 lr: 0.0000\n",
            "Step: 18000 Loss: 159.3356 lr: 0.0000\n",
            "Step: 18500 Loss: 159.0709 lr: 0.0000\n",
            "Step: 19000 Loss: 158.6260 lr: 0.0000\n",
            "Step: 19500 Loss: 158.0818 lr: 0.0000\n",
            "Step: 20000 Loss: 158.6034 lr: 0.0000\n",
            "Step: 20500 Loss: 158.4807 lr: 0.0000\n",
            "Step: 21000 Loss: 158.3486 lr: 0.0000\n",
            "Step: 21500 Loss: 158.6155 lr: 0.0000\n",
            "Step: 22000 Loss: 158.5700 lr: 0.0000\n",
            "Step: 22500 Loss: 158.6655 lr: 0.0000\n",
            "Step: 23000 Loss: 158.9917 lr: 0.0000\n",
            "Step: 23500 Loss: 158.7230 lr: 0.0000\n",
            "Step: 24000 Loss: 158.6454 lr: 0.0000\n",
            "Step: 24500 Loss: 159.4659 lr: 0.0000\n",
            "Step: 25000 Loss: 158.4800 lr: 0.0000\n",
            "Step: 25500 Loss: 159.5890 lr: 0.0000\n",
            "Step: 26000 Loss: 158.8137 lr: 0.0000\n",
            "Step: 26500 Loss: 158.8485 lr: 0.0000\n",
            "Step: 27000 Loss: 158.6744 lr: 0.0000\n",
            "Step: 27500 Loss: 158.1890 lr: 0.0000\n",
            "Step: 28000 Loss: 159.6759 lr: 0.0000\n",
            "Step: 28500 Loss: 159.0758 lr: 0.0000\n",
            "Step: 29000 Loss: 158.6426 lr: 0.0000\n",
            "Step: 29500 Loss: 159.4818 lr: 0.0000\n",
            "Step: 30000 Loss: 159.1815 lr: 0.0000\n",
            "Step: 30500 Loss: 158.3649 lr: 0.0000\n",
            "Step: 31000 Loss: 158.8646 lr: 0.0000\n",
            "Step: 31500 Loss: 158.8750 lr: 0.0000\n",
            "Step: 32000 Loss: 158.7011 lr: 0.0000\n",
            "Step: 32500 Loss: 158.3464 lr: 0.0000\n",
            "Step: 33000 Loss: 158.5476 lr: 0.0000\n",
            "Step: 33500 Loss: 158.9490 lr: 0.0000\n",
            "Step: 34000 Loss: 158.3123 lr: 0.0000\n",
            "Step: 34500 Loss: 160.0973 lr: 0.0000\n",
            "Step: 35000 Loss: 158.7748 lr: 0.0000\n",
            "Epoch 2 Total Mean Loss : 158.7612\n",
            "*****Epoch 2 Train Finished*****\n",
            "\n",
            "*****Epoch 2 Saving Embedding...*****\n",
            "*****Epoch 2 Embedding Saved at ./word2vec_wiki/w2v_2.txt*****\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 학습\n",
        "w2v.train(train_dataloader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3uTIm4vJ4WSp"
      },
      "source": [
        "### 유사한 단어 확인\n",
        "- 사전에 존재하는 단어들과 유사한 단어를 검색해보자. Gensim 패키지는 유사 단어 외에도 단어간의 유사도를 계산하는 여러 함수를 제공한다. 실험을 통해 word2vec의 한계점을 발견했다면 아래에 markdown으로 작성해보자. \n",
        "- [Gensim 패키지 document](https://radimrehurek.com/gensim/models/keyedvectors.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:47:59.605389Z",
          "start_time": "2022-02-19T14:47:59.368925Z"
        },
        "id": "AKpBuVlP4WSp"
      },
      "outputs": [],
      "source": [
        "import gensim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:49:06.590460Z",
          "start_time": "2022-02-19T14:49:05.174241Z"
        },
        "id": "AWTCodimsAq8"
      },
      "outputs": [],
      "source": [
        "word_vectors = gensim.models.KeyedVectors.load_word2vec_format('/content/drive/MyDrive/word2vec_wiki/w2v_1.txt', binary=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:49:11.324372Z",
          "start_time": "2022-02-19T14:49:11.315429Z"
        },
        "id": "MLMh_evrsAq9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "outputId": "c021ac47-747c-453f-f210-55148b7b2d7d"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-99-a367903c7149>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mword_vectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpositive\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mmost_similar\u001b[0;34m(self, positive, negative, topn, restrict_vocab, indexer)\u001b[0m\n\u001b[1;32m    533\u001b[0m                     \u001b[0mall_words\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 535\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cannot compute similarity with no input\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    536\u001b[0m         \u001b[0mmean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munitvec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mREAL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: cannot compute similarity with no input"
          ]
        }
      ],
      "source": [
        "word_vectors.most_similar(positive=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "word2vec의 한계점은?"
      ],
      "metadata": {
        "id": "X8lc8NQe4cT2"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Hani_Bae_Week3_1_assignment_0313.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}